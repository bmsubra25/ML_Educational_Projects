{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "KJlLX-fdzSBn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e7Zv6ntly-wf"
      },
      "outputs": [],
      "source": [
        "# This will be much more dense in terms of imports\n",
        "import torch\n",
        "import torch.nn.functional\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from datasets import load_dataset\n",
        "from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR\n",
        "import matplotlib.pyplot\n",
        "import contextlib\n",
        "from torch.utils.checkpoint import checkpoint\n",
        "from torch.optim import AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the device"
      ],
      "metadata": {
        "id": "YL2_VGyLzYho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this runtime, a GPU is almost mandatory, so please connect to a GPU runtime of some kind if you haven't already\n",
        "device = (torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\"))"
      ],
      "metadata": {
        "id": "kUj-ju8szZZl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classes(Attention_Head, Multi_Headed_Attention, Transformer_Block)"
      ],
      "metadata": {
        "id": "sO6AYEqmzTLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention_Head(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, head_embedding_size):\n",
        "    super().__init__()\n",
        "    self.head_embedding_size = head_embedding_size\n",
        "    # For the following classes, we will use torch.nn.Linear instead of standard matrices. The reason for this is that nn.Linear is optimized for GPU training\n",
        "    # nn.Linear without biases functions the exact same as a normal matrix tensor. nn.Linear with biases functions equivalently to one layer of an MLP pre-activation\n",
        "    self.query = torch.nn.Linear(embedding_size, head_embedding_size, bias = False)\n",
        "    self.key = torch.nn.Linear(embedding_size, head_embedding_size, bias = False)\n",
        "    self.value = torch.nn.Linear(embedding_size, head_embedding_size, bias = False)\n",
        "  def forward(self, embeddings, mask):\n",
        "    # These multiply the query, key, and value matrices with the embeddings, producing embeddings that are scaled down to the head_embedding_size\n",
        "    query_vectors = self.query(embeddings)\n",
        "    key_vectors = self.key(embeddings)\n",
        "    value_vectors = self.value(embeddings)\n",
        "    # Now we dot the list of query vectors with the list of key vectors and scale them down\n",
        "    key_query_product = torch.matmul(query_vectors,key_vectors.transpose(-2,-1)) / math.sqrt(self.head_embedding_size)\n",
        "    # Here, we apply a causal mask to the key_query_product to prevent the future tokens from providing information to past ones\n",
        "    key_query_product = key_query_product.masked_fill(mask, float(\"-inf\"))\n",
        "    # Now we apply softmax, then dot the key_query_product with the value vectors to create context vectors\n",
        "    key_query_product = torch.nn.functional.softmax(key_query_product, dim = -1).to(value_vectors.dtype)\n",
        "    return torch.matmul(key_query_product, value_vectors)\n",
        "  # Note: We don't have an optimization method here, as we are going to use Pytorch's \"Optimizer Adam\"\n",
        "\n",
        "class Multi_Headed_Attention(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, num_heads):\n",
        "    super().__init__()\n",
        "    # We instantiate with module list in order to access params with a Pytorch optimizer later on\n",
        "    self.attentionHeads = torch.nn.ModuleList([\n",
        "      Attention_Head(embedding_size, embedding_size // num_heads) for _ in range(num_heads)\n",
        "    ])\n",
        "    # torch.nn.Linear is identical to the Layers_Dense class from the other notebooks, but it has faster performance on GPUs due to optimization in PyTorch\n",
        "    self.linear = torch.nn.Linear(embedding_size,embedding_size, bias = True)\n",
        "\n",
        "  def forward(self, inputs, mask):\n",
        "    # We define a list that will eventually contain all the outputs of the attention layers\n",
        "    attn_outputs = []\n",
        "    # We compute attention with all the heads, then concatenate them into one list\n",
        "    for head in self.attentionHeads:\n",
        "      attn_outputs.append(head.forward(inputs,mask))\n",
        "    attn_outputs = torch.cat(attn_outputs, dim = -1)\n",
        "    # We apply a linear operation\n",
        "    return self.linear(attn_outputs)\n",
        "\n",
        "class Transformer_Block(torch.nn.Module):\n",
        "  def __init__(self, embedding_size, num_heads):\n",
        "    super().__init__()\n",
        "    # We define layer dropout here, so that we can prevent overfitting and have the model make better use of params\n",
        "    self.dropout = torch.nn.Dropout(p = 0.1)\n",
        "    # Two layer norms for two residual connection additions\n",
        "    self.layer_norm_1 = torch.nn.LayerNorm(embedding_size, eps = 1e-5)\n",
        "    self.layer_norm_2 = torch.nn.LayerNorm(embedding_size, eps = 1e-5)\n",
        "    # Its conventional to have linear layers perform an operation that scales embeddings to a higher dimensionality, apply an activation function, and scale them back down\n",
        "    # In theory, this should allow the embeddings to recieve higher information across more dimensions than the embeddings allow, before scaling them back down\n",
        "    self.linear_i = torch.nn.Linear(embedding_size,4*embedding_size, bias = True)\n",
        "    self.linear_f = torch.nn.Linear(4*embedding_size,embedding_size, bias = True)\n",
        "    # We define our Multi_Headed_Attention block here\n",
        "    self.attention = Multi_Headed_Attention(embedding_size,num_heads)\n",
        "\n",
        "  def forward(self, inputs, mask):\n",
        "    # In this implementation we make use of pre-norming before attention and passing through feedforward layers\n",
        "    inputs = inputs + self.dropout(self.attention(self.layer_norm_1(inputs),mask))\n",
        "    # We pass through an activation function that often yields great results for LLM training, called GeLu\n",
        "    # GeLu itself is a function that scales negative values similarly to ReLu, except with a softer cutoff\n",
        "    inputs = inputs + self.dropout(self.linear_f(torch.nn.functional.gelu(self.linear_i(self.layer_norm_2(inputs)), approximate='tanh')))\n",
        "    # We return the updated embeddings with added context\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "Dt9JMKg0zlzY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating and pre-processing our dataset"
      ],
      "metadata": {
        "id": "UkcF3ePI8fLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are params you can experiment with. A smaller corpus is easier to fit on, but might not produce equivalent intelligibility to a larger one. Vocab is similar here\n",
        "len_corpus = 200_000_000\n",
        "vocab_size = 17500\n",
        "current_chars = 0\n",
        "target_chars = len_corpus\n",
        "subset_text = []\n",
        "# We load our dataset here. My goal when desigining this was coherent phrase completion, so I used the \"bookcorpusopen\", a corpus made of compiled book passages\n",
        "ds = load_dataset(\"rojagtap/bookcorpus\", split=\"train\", streaming=True)\n",
        "# Creating the data\n",
        "for chunk in ds:\n",
        "  text = chunk[\"text\"]\n",
        "  # We get chunks from our dataset and append them to our subset of the text\n",
        "  if current_chars + len(text) > target_chars:\n",
        "    subset_text.append(text[:target_chars - current_chars])\n",
        "    break\n",
        "  subset_text.append(text)\n",
        "  current_chars += len(text)\n",
        "\n",
        "# Cleaning\n",
        "added = set()\n",
        "cleaned = []\n",
        "\n",
        "# We clean chunks in our subset and append them\n",
        "for chunk in subset_text:\n",
        "  chunk = chunk.strip()\n",
        "  if len(chunk) >= 10:\n",
        "    if chunk in added:\n",
        "      continue\n",
        "    else:\n",
        "      cleaned.append(chunk)\n",
        "      added.add(chunk)\n",
        "\n",
        "# Here, we create a tokenizer and create our vocab\n",
        "tokenizer = Tokenizer(BPE()); tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(vocab_size=vocab_size, show_progress=True)\n",
        "random.shuffle(cleaned)\n",
        "# Trains the tokenizer\n",
        "tokenizer.train_from_iterator(iter(cleaned), trainer=trainer)\n",
        "assert len(tokenizer.get_vocab()) > 0\n",
        "print(len(tokenizer.get_vocab()))\n",
        "\n",
        "# We create our dataset here\n",
        "ids = tokenizer.encode(\"\\n\\n\".join(cleaned)).ids\n",
        "print(f\"Total number of tokens in dataset: {len(ids):,}\")\n",
        "data = torch.tensor(ids,dtype = torch.long).cpu()\n",
        "n_train = int(0.9*data.numel())\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:]"
      ],
      "metadata": {
        "id": "wgNTUckw9YdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a batching method"
      ],
      "metadata": {
        "id": "FwX113pRAqO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# minGPT by Andrej Karpathy\n",
        "# https://github.com/karpathy/minGPT\n",
        "# MIT License\n",
        "# This block of code is inspired by Andrej Karpathy's \"Let's Build GPT from Scratch\"\n",
        "# The shape of a training or validation example will be batch size x sequence len x embedding dimensionality\n",
        "def get_batch(split, batch_size, seq_len):\n",
        "  data_split = train_data if split =='train' else val_data\n",
        "  ix = torch.randint(0, len(data_split)-seq_len-1, (batch_size,), device='cpu')\n",
        "  offsets = torch.arange(seq_len, device='cpu').unsqueeze(0)\n",
        "  # We do this in the style of the target sequence being the input sequence shifted by one\n",
        "  # This allows us to get much more training examples out of one training one, for example if the input sequence was\n",
        "  # \"The dog was happy that\"\n",
        "  # The target sequence would be:\n",
        "  # \"dog was happy that he\"\n",
        "  # We get predictions based on \"The\", \"The dog\", \"The dog was\", \"The dog was happy\", and \"The dog was happy that\"\n",
        "  x = data_split[ix.unsqueeze(1) + offsets].pin_memory()\n",
        "  y = data_split[ix.unsqueeze(1) + offsets + 1].pin_memory()\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "Sn1WxsjvA8EK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Class"
      ],
      "metadata": {
        "id": "qLqIv8S1BR-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(torch.nn.Module):\n",
        "  def __init__(self,embedding_size,num_heads,num_layers, lr):\n",
        "    super().__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_heads = num_heads\n",
        "    # Wraps everything in a ModuleList so that params can be accessed later by the optimizer\n",
        "    self.layers = torch.nn.ModuleList([\n",
        "        Transformer_Block(embedding_size,num_heads) for _ in range(num_layers)\n",
        "    ])\n",
        "    # Layer Norm for norming the embeddings\n",
        "    self.layer_norm = torch.nn.LayerNorm(embedding_size)\n",
        "    # Stores loss history for use with matplotlib\n",
        "    self.training_loss_history = []\n",
        "    self.val_loss_history = []\n",
        "    # Creates the embedding table and biases for the final linear projection\n",
        "    # You could get similar functionality out of simply creating a Tensor of the same shape and indexing but nn.Embedding is specialized for \"sparse updates\", when only a few\n",
        "    # Vectors within an embedding table get updated. Because of this, using nn.embedding can save you memory and time\n",
        "    # Ex: self.embedding_table = torch.nn.Parameter(torch.randn(vocab_size,embedding_size), requires_grad = True).to(device)\n",
        "    self.embedding_table = torch.nn.Embedding(vocab_size, embedding_size).to(device)\n",
        "    self.vocab_bias = torch.nn.Parameter(torch.zeros(vocab_size), requires_grad=True).to(device)\n",
        "    # Instantiates an \"Optimizer Adam\" instance, making use of learning rate adjustments over time\n",
        "    self.optimizer_lr = lr\n",
        "    fused = (device.type == \"cuda\")\n",
        "    self.optimizer = torch.optim.AdamW(self.parameters(),lr=lr,betas=(0.9, 0.95),eps=1e-8,weight_decay=0.0,fused=fused)\n",
        "\n",
        "  def positional_encoding(self, seq_len, embedding_size):\n",
        "     # If you want to learn about this specific kind of PE, read the original Transformer paper \"Attention is All You Need\"\n",
        "    pos_encoding = torch.zeros(seq_len,embedding_size)\n",
        "    pos = torch.arange(0,seq_len, dtype = torch.float).unsqueeze(1)\n",
        "    div_term = torch.exp(torch.arange(0, embedding_size, 2).float() * -(math.log(10000.0) / embedding_size))\n",
        "    # Applies the formula from the paper to even and odd indices\n",
        "    pos_encoding[:, 0::2] = torch.sin(pos * div_term)\n",
        "    pos_encoding[:, 1::2] = torch.cos(pos * div_term)\n",
        "    return pos_encoding\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # We save these shapes for later use\n",
        "    batch_size, seq_len = inputs.shape\n",
        "    # Accesses embeddings through token indices and adds positional encoding\n",
        "    embeddings = self.embedding_table(inputs) + self.pos_encoding[:inputs.size(1)]\n",
        "    # We compute a causal mask here and pass it to the transformer blocks\n",
        "    mask = torch.triu(torch.ones(inputs.size(1), inputs.size(1), dtype=torch.bool, device=inputs.device), diagonal=1)\n",
        "    mask = mask.unsqueeze(0)\n",
        "    # We pass the embeddings through all of the model's layers\n",
        "    for block in self.layers:\n",
        "      embeddings = block.forward(embeddings,mask)\n",
        "    # We norm the embeddings, then project them to vocab_size\n",
        "    # We use weight tying here(using the same weights for multiple operations), as it saves memory greatly and leads to better performance\n",
        "    logits = torch.matmul(embeddings, self.embedding_table.weight.T)+ self.vocab_bias\n",
        "    # Each row of the final logits has shape [batch_size, seq_len, vocab_size]\n",
        "    # Each position contains information about the corresponding token in the input sequence\n",
        "    # These logits are raw scores predicting the likelihood of each token being the next one (before softmax)\n",
        "    return logits\n",
        "\n",
        "  def train_model(self, num_batches, batch_size, seq_len):\n",
        "    # This puts the model in training mode, where layer dropout is used\n",
        "    self.train()\n",
        "    # We set a warmup time so that we can implement learning rate warmup and decay, useful features for optimizing training\n",
        "    warmup_steps = 1000\n",
        "    hold_until = int(0.35 * num_batches)\n",
        "    # Defines the warmup, which allows the model to slowly increase its lr so that it uses its peak lr during the most usefl parts of training\n",
        "    warmup = LambdaLR(self.optimizer, lr_lambda=lambda s: (s+1)/max(1,warmup_steps))\n",
        "    # Defines how long the model keeps its lr\n",
        "    flat = LambdaLR(self.optimizer, lr_lambda=lambda s: 1.0)\n",
        "    # Defines when the learning rate decays, allowing for more pecise gradient adjustments that don't bounce around minimums\n",
        "    min_lr = 1e-4\n",
        "    cosine = CosineAnnealingLR(self.optimizer, T_max=max(1, num_batches - hold_until), eta_min=min_lr)\n",
        "    # Composite scheduler\n",
        "    scheduler = SequentialLR(self.optimizer, [warmup, flat, cosine], milestones=[warmup_steps, hold_until])\n",
        "    # Now, we will define some features in order to conserve memory during training\n",
        "    is_cuda = (device == \"cuda\") # If we have a GPU runtime enabled\n",
        "    # We make use of autocast_ctx in order to save memory during training by casting float32s to float16s when safe\n",
        "    autocast_ctx = (lambda: torch.amp.autocast(device_type=\"cuda\", dtype=torch.bfloat16)) if is_cuda else contextlib.nullcontext\n",
        "    # Creates a positional encoding tensor so we don't have to reuse it\n",
        "    self.pos_encoding = self.positional_encoding(seq_len, self.embedding_size).to(device)\n",
        "    for idx in range(num_batches):\n",
        "      # Gets batches for training\n",
        "      xb, yb = get_batch(\"train\", batch_size, seq_len)\n",
        "      xb = xb.to(device, non_blocking=True)\n",
        "      yb = yb.to(device, non_blocking=True)\n",
        "      # Sets loss to none outside of the loop so that we can use it as a reference later\n",
        "      loss = None\n",
        "      try:\n",
        "        # Uses autocast_ctx during the forward process\n",
        "        with autocast_ctx():\n",
        "          logits = self.forward(xb)\n",
        "          assert logits.shape[:-1] == yb.shape, f\"Shape mismatch: {logits.shape} vs {yb.shape}\"\n",
        "          loss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), yb.view(-1))\n",
        "\n",
        "      except Exception as e:\n",
        "            print(f\"Error at batch {idx}: {e}\")\n",
        "            continue\n",
        "      if loss is not None:\n",
        "        # Performs optimization\n",
        "        self.optimizer.zero_grad(set_to_none = True)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.parameters(), 2.0)\n",
        "        self.optimizer.step()\n",
        "        # Steps with the scheduler, adjusting lr if needed\n",
        "        scheduler.step()\n",
        "        # Appends current loss for plotting later\n",
        "        self.training_loss_history.append(loss.item())\n",
        "        if idx%50 == 0:\n",
        "          print(f\"[train] {idx}: {loss.item():.4f}\")\n",
        "        # Validation every 200 steps\n",
        "        if idx%200 == 0 :\n",
        "          # Sets to eval mode so there is no dropout\n",
        "          self.eval()\n",
        "          with torch.no_grad(), autocast_ctx():\n",
        "            # Gets a val batch\n",
        "            vx, vy = get_batch(\"val\", batch_size, seq_len)\n",
        "            vx = vx.to(device, non_blocking=True)\n",
        "            vy = vy.to(device, non_blocking=True)\n",
        "\n",
        "            v_logits = self.forward(vx)\n",
        "            v_loss = torch.nn.functional.cross_entropy(\n",
        "                v_logits.view(-1, v_logits.size(-1)), vy.view(-1)\n",
        "            )\n",
        "            # Appends to the val_loss history10\n",
        "            self.val_loss_history.append(v_loss.item())\n",
        "            print(f\"[val] {idx}: {v_loss.item():.4f}\")\n",
        "          # Sets back to training mode\n",
        "          self.train()\n",
        "  # Here we implement topk generation, where instead of simply choosing the most likely output, the model chooses between the top 50. This often leads to less repetitive sentences\n",
        "  # and more coherent sentence completion\n",
        "  def generate(self,prompt, tokenizer, max_new_tokens, k):\n",
        "    # We encode the prompt and add a batch dimension\n",
        "    prompt = torch.tensor(tokenizer.encode(prompt).ids, dtype = torch.long, device = device).unsqueeze(0)\n",
        "    # We progressively add new tokens till we reach the max new, picking from the topk tokens\n",
        "    with torch.no_grad():\n",
        "      for i in range(max_new_tokens):\n",
        "        self.pos_encoding = self.positional_encoding(prompt.shape[1], self.embedding_size).to(device)\n",
        "        # We forward the prompt through the model\n",
        "        logits = self.forward(prompt)\n",
        "        # We slice the sequence to the last embedding\n",
        "        next_token_probs = logits[:,-1,:]\n",
        "        top_k, top_k_indices = torch.topk(next_token_probs,k)\n",
        "        # We take all the tokens that aren't in the top_k values out of the equation\n",
        "        next_token_probs[next_token_probs < top_k[:, -1, None]] = float(\"-inf\")\n",
        "        # We create a probability distribution using the rest\n",
        "        probs = torch.nn.functional.softmax(next_token_probs, dim = -1)\n",
        "        # We choose from the tokens and append them to the sequence\n",
        "        next_token = torch.multinomial(probs, num_samples=1, replacement=False)\n",
        "        prompt = torch.cat((prompt,next_token), dim = 1)\n",
        "\n",
        "    return tokenizer.decode(prompt.squeeze(0).tolist())"
      ],
      "metadata": {
        "id": "dXNUSmeUBYwC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model"
      ],
      "metadata": {
        "id": "w0G6NVXkHjLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 512 embedding dim, 8 attention heads per layer, 7 layers, learning rate of 5e-4\n",
        "transformer = Transformer(512,8,7,5e-4)\n",
        "# We put the transformer on cuda so it can make use of the gpu\n",
        "transformer.to(device)\n",
        "# We adjust some extra params so that we can optimize for speed\n",
        "if device.type == \"cuda\":\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "    transformer = torch.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n",
        "\n",
        "# This allows you to see all the params included in the model\n",
        "for name, param in transformer.named_parameters():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "id": "Rr0dYaVbHi6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "Dwbpg9_HJNvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transformer.train_model(20000,128,256)"
      ],
      "metadata": {
        "id": "LjWobK5eJOrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a saved model"
      ],
      "metadata": {
        "id": "UdaHROXsH5QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Models aren't loaded perfectly after saving, so a small amount of training might be necessary to get the model back to its original performance\n",
        "\"\"\"\n",
        "transformer = Transformer(512,8,7,5e-4) # Make sure the model is instantiated the same as your saved run\n",
        "tokenizer = Tokenizer.from_file(\"my_tokenizer.json\")\n",
        "if device.type == \"cuda\":\n",
        "  torch.set_float32_matmul_precision(\"high\")\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  transformer = torch.compile(transformer, mode=\"max-autotune\", fullgraph=True)\n",
        "transformer.load_state_dict(torch.load(\"transformer.pt\", map_location=device, weights_only=False))\n",
        "transformer.to(device)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "nALGT0PKICVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving a pre-trained model"
      ],
      "metadata": {
        "id": "1CaCQ-OxIbf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(transformer.state_dict(), \"transformer.pt\", _use_new_zipfile_serialization=True)\n",
        "tokenizer.save(\"my_tokenizer.json\")"
      ],
      "metadata": {
        "id": "2SU8rCezIdiP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompting"
      ],
      "metadata": {
        "id": "HOAgdRuhJKVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# With the params I have saved, the model can do short sentence completion but may become\n",
        "# more incoherent as sentence length increases.\n",
        "prompt = \"Hello, how are you doing? I'm doing fine, responded\"\n",
        "response = prompt[0] + transformer.generate(prompt, tokenizer, 20, 25)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "kCZNQafjQYZS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
