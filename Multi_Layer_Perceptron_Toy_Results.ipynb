{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "_n138Rmri7ix"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VYpSnDD_i2Ny"
      },
      "outputs": [],
      "source": [
        "# Imports kept to a bare minimum\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP Layers_Dense Class"
      ],
      "metadata": {
        "id": "_EA6THWIi8rE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This class represents one layer in a multi-layer perceptron\n",
        "class Layers_Dense:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    # Weights defined to be congruent with inputs from each previous layer\n",
        "    self.weights = torch.rand(n_inputs,n_neurons)\n",
        "    # Biases defined with dims so that they can be added directly to weights after\n",
        "    self.biases = torch.rand((1,n_neurons))\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    # We need to save inputs to compute gradients later\n",
        "    self.inputs = inputs\n",
        "    # Matmul used for vectorization, so you can train on batched data\n",
        "    self.outputs = torch.matmul(self.inputs,self.weights) + self.biases\n",
        "    # Note: we do not pass through the activation function here. This is to give you flexibility as to whether you want to use it or not. Pytorch's nn.Linear is built in a similar way\n",
        "    return self.outputs\n",
        "  def backwards(self, dvalues):\n",
        "    # \"dvalues\" represents dL/dZ\n",
        "    # In order to calculate dL/dw, we multiply dL/dZ * dZ/dw, which is just the total input activation * dvalues\n",
        "    self.dweights = torch.matmul(self.inputs.T, dvalues)\n",
        "    # The biases grad is technically 1, but it's convention to just sum up dvalues\n",
        "    self.dbiases = torch.sum(dvalues, axis=0, keepdims=True)\n",
        "    # When we pass gradients backwards, we need to compute the gradient of loss in respect to them\n",
        "    # We get net activation from the previous layers as our inputs, so we have to compute dZ/dA here. That is just dvalues * weights\n",
        "    self.dinputs = torch.matmul(dvalues,self.weights.T)"
      ],
      "metadata": {
        "id": "cjiDPfhQi63j"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "YEUXXtK9jBtG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Relu is just a function that returns the same value if it is greater than or equal to 0 or 0 if it is less\n",
        "def relu(inputs):\n",
        "    return inputs.clamp(min=0.0)\n",
        "\n",
        "# Relu's derivative is 1 if the inputs are > 0 and 0 if they are less than or equal to it\n",
        "def relu_deriv(inputs):\n",
        "    return (inputs > 0).float()\n",
        "\n",
        "# Using sigmoid for the final layer, a simple nonlinear activation function that squishes values between 0 and 1\n",
        "def sigmoid(inputs):\n",
        "  return 1/(1+torch.exp(-inputs))\n",
        "\n",
        "# Returns the derivative of sigmoid\n",
        "def sigmoid_deriv(inputs):\n",
        "  return sigmoid(inputs)*(1-sigmoid(inputs))"
      ],
      "metadata": {
        "id": "P3trEIgCjCvd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Toy Dataset"
      ],
      "metadata": {
        "id": "pGdVM4p2jFiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For the sake of keeping things concise, this example will train the model on a simple problem\n",
        "# We have 3 bits A B and C. When C is 1, A and B behave like an OR gate. When C is 0, they behave like an AND gate\n",
        "\n",
        "# Training data\n",
        "X = torch.tensor([\n",
        "    [0.0,0.0,0.0],\n",
        "    [0.0,0.0,1.0],\n",
        "    [1.0,0.0,0.0],\n",
        "    [1.0,0.0,1.0],\n",
        "    [0.0,1.0,0.0],\n",
        "    [0.0,1.0,1.0],\n",
        "    [1.0,1.0,0.0],\n",
        "    [1.0,1.0,1.0],]\n",
        ")\n",
        "# Predictions\n",
        "y = torch.tensor([\n",
        "    [0.], # A=0,B=0,C=0 -> AND: 0 AND 0 = 0\n",
        "    [0.],  # A=0,B=0,C=1 -> OR: 0 OR 0 = 0\n",
        "    [0.],  # A=1,B=0,C=0 -> AND: 1 AND 0 = 0\n",
        "    [1.],  # A=1,B=0,C=1 -> OR: 1 OR 0 = 1\n",
        "    [0.],  # A=0,B=1,C=0 -> AND: 0 AND 1 = 0\n",
        "    [1.],  # A=0,B=1,C=1 -> OR: 0 OR 1 = 1\n",
        "    [1.],  # A=1,B=1,C=0 -> AND: 1 AND 1 = 1\n",
        "    [1.],  # A=1,B=1,C=1 -> AND: 1 AND 1 = 1\n",
        "])\n"
      ],
      "metadata": {
        "id": "qlV-iO33jtJd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "U7fs7JQFjuzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define MLP layers outside of a container class for illustrative purposes\n",
        "layer1 = Layers_Dense(3,3)\n",
        "layer2 = Layers_Dense(3,2)\n",
        "layer3 = Layers_Dense(2,2)\n",
        "layer4 = Layers_Dense(2,1)\n",
        "\n",
        "# Loss function\n",
        "# We use binary cross-entropy in order for two class output. For more than two, use categorical cross-entropy\n",
        "\n",
        "def binary_cross_entropy_loss(y_true, y_pred):\n",
        "    epsilon = 1e-7  # To avoid log(0)\n",
        "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
        "    return -(y_true * torch.log(y_pred) + (1 - y_true) * torch.log(1 - y_pred))\n",
        "\n",
        "def bce_deriv(y_true, y_pred):\n",
        "    epsilon = 1e-7\n",
        "    y_pred = torch.clamp(y_pred, epsilon, 1 - epsilon)\n",
        "    return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
        "\n",
        "# Epochs are just the number of total runs where the model trains on all possible examples\n",
        "# Note: You usually won't need 100 epochs\n",
        "for j in range(0,100):\n",
        "  epoch_loss = []\n",
        "  for example in range(0,8):\n",
        "    # Forward pass for layer1\n",
        "    logits1 = layer1.forward(X[example].unsqueeze(0))\n",
        "    outputs1 = relu(logits1)\n",
        "\n",
        "    # Forward pass for layer2\n",
        "    logits2 = layer2.forward(outputs1)\n",
        "    outputs2 = relu(logits2)\n",
        "\n",
        "    # Forward pass for layer3\n",
        "    logits3 = layer3.forward(outputs2)\n",
        "    outputs3 = relu(logits3)\n",
        "\n",
        "    # Forward pass for layer2(logits saved)\n",
        "    logits4 = layer4.forward(outputs3)\n",
        "    outputs4 = sigmoid(logits4)\n",
        "    # Loss and backprop\n",
        "    y_true_single = y[example].unsqueeze(0)\n",
        "    loss = binary_cross_entropy_loss(y_true_single,outputs4)\n",
        "    epoch_loss.append(loss.item())\n",
        "    # First, dL/dA is calculated with BCE loss\n",
        "    dloss = bce_deriv(y_true_single,outputs4) # dL/dA\n",
        "    dactivation4 = sigmoid_deriv(logits4)#dA/dZ\n",
        "    # Backpropagation\n",
        "    layer4.backwards(dloss*dactivation4) # passing in dL/dZ\n",
        "    # Now for layer3\n",
        "    dactivation3 = relu_deriv(logits3)\n",
        "    layer3.backwards(layer4.dinputs*dactivation3)\n",
        "    # Now for layer2\n",
        "    dactivation2 = relu_deriv(logits2)\n",
        "    layer2.backwards(layer3.dinputs*dactivation2)\n",
        "    # Now for layer1\n",
        "    dactivation1 = relu_deriv(logits1)\n",
        "    layer1.backwards(layer2.dinputs*dactivation1)\n",
        "    # Now we optimize with a learning rate of 0.05\n",
        "    learningRate = 0.05\n",
        "    layer1.weights -= learningRate*layer1.dweights\n",
        "    layer1.biases -= learningRate*layer1.dbiases\n",
        "    layer2.weights -= learningRate*layer2.dweights\n",
        "    layer2.biases -= learningRate*layer2.dbiases\n",
        "    layer3.weights -= learningRate * layer3.dweights\n",
        "    layer3.biases -= learningRate * layer3.dbiases\n",
        "    layer4.weights -= learningRate * layer4.dweights\n",
        "    layer4.biases -= learningRate * layer4.dbiases\n",
        "  print(f\"Epoch: {j} Loss: {torch.mean(torch.tensor(epoch_loss))}\")"
      ],
      "metadata": {
        "id": "pVnOuG8DjyZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "VqU33Hnpjz8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a modelforward method so that we can quickly forward inputs\n",
        "def modelforward(layer1,layer2,layer3,layer4,X):\n",
        "  outputs1 = relu(layer1.forward(X))\n",
        "  outputs2 = relu(layer2.forward(outputs1))\n",
        "  outputs3 = relu(layer3.forward(outputs2))\n",
        "  outputs4 = sigmoid(layer4.forward(outputs3))\n",
        "  return outputs4\n",
        "# Example test output. There are a finite number of possible cases, and the model has memorized them all\n",
        "print(modelforward(layer1,layer2,layer3,layer4,X).unsqueeze(0))\n",
        "# MLPs are used instrumentally in larger ML Architectures(GNNs, SNNs, Transformers, etc.) Once you understand how they work, you can move on to the larger models in the repo\n",
        "# The code I wrote for the other models will be more heavily abstracted(using pytorch's autograd and other methods)"
      ],
      "metadata": {
        "id": "iEBOdChsj4mU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a421bf5a-9f94-4999-9b65-cc70e0c81eae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0263],\n",
            "         [0.0171],\n",
            "         [0.0163],\n",
            "         [0.9965],\n",
            "         [0.0192],\n",
            "         [0.9971],\n",
            "         [0.9969],\n",
            "         [1.0000]]])\n"
          ]
        }
      ]
    }
  ]
}
