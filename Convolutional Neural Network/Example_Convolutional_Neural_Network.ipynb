{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "cpuoEBfvUbTr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K52ClJluUUqB"
      },
      "outputs": [],
      "source": [
        "# PLEASE CONNECT TO GOOGLE COLAB'S FREE T4 GPU RUNTIME BEFORE YOU RUN THIS CELL OR THE FOLLOWING ONES\n",
        "# Utils imports\n",
        "import torch\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "# Dataset imports\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.datasets as data\n",
        "from tensorflow.keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layers_Dense Class and Flattener Class(For MLP Layers and 1D Flattening)"
      ],
      "metadata": {
        "id": "9BFWQ62uUc1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Layers_Dense(torch.nn.Module):\n",
        "  # torch.nn.Module used in order for convenient parameter access and gradient zeroing.\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    super().__init__()\n",
        "    # We wrap the tensors in a Parameter class so that we can access them through other classes that use Layers_Dense.\n",
        "    self.weights = torch.nn.Parameter(torch.randn(n_inputs,n_neurons)*0.01,requires_grad = True)\n",
        "    self.biases = torch.nn.Parameter((torch.zeros(1,n_neurons)),requires_grad=True)\n",
        "  def forward(self,inputs):\n",
        "    self.inputs = inputs\n",
        "    # We're going to use pytorch's autograd to do SGD, so we don't need to save inputs.\n",
        "    self.outputs = torch.matmul(self.inputs,self.weights) + self.biases\n",
        "    return self.outputs\n",
        "  def backwardsAndOptimize(self,learningRate):\n",
        "    # We allow a learningRate pass from here so that we can still do manual gradient descent, but it will recieve inputs from higher and more abstracted classes\n",
        "    with torch.no_grad():\n",
        "      # Pytorch computes gradients automatically with a computational graph that tracks any changes to tensors. This also applies to gradients, so you have to explicitly declare\n",
        "      # That there will be no gradient tracking for this operation.\n",
        "      self.weights -= learningRate * self.weights.grad\n",
        "      self.biases -= learningRate * self.biases.grad\n",
        "      # How we can access gradients without passing in loss will be explained later.\n",
        "\n",
        "class Flattener(torch.nn.Module):\n",
        "  # Exists as a wrapper for the sake of being fed as a part of the CNN\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,inputs):\n",
        "    batch_size = inputs.shape[0]\n",
        "    # Flattens the inputs and returns them\n",
        "    outputs = inputs.reshape(batch_size,-1)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "8yJWP0FzUwFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Max-Pooling Class"
      ],
      "metadata": {
        "id": "kQlhfE0TU2KQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this code example, we will use Max-Pooling specifically for downsizing images\n",
        "class Max_Pooling(torch.nn.Module):\n",
        "  def __init__(self,dimx,dimy):\n",
        "    super().__init__()\n",
        "    # We declare the dimensions for the pooling blocks beforehand\n",
        "    self.dimx = dimx\n",
        "    self.dimy = dimy\n",
        "  def forward(self,inputs):\n",
        "    # We save variables that are part of the input's shape, as we have to operate on the same tensor for gradient tracking.\n",
        "    # If you do not operate on the same tensor, your gradients will not flow, and you won't be able to do SGD for your kernels.\n",
        "    # Fortunately, Pytorch has built in methods that are made for pooling and kernelling while allowing gradient flow\n",
        "    batch_size, color_channels, height, width = inputs.shape\n",
        "    # Pytorch's tensor unfolding method splits an input tensor into rectangular chunks among certain dims with params (dimension, section length, step_size)\n",
        "    blocks = inputs.unfold(2,self.dimy,self.dimy).unfold(3,self.dimy,self.dimy)\n",
        "    blocks = blocks.contiguous().view(batch_size,color_channels,blocks.size(2),blocks.size(3),-1)\n",
        "    # The following code reduces each patch to it's maximum value, performing the pooling operation. When you call the maxing operation, you also get indices for these maxes returned.\n",
        "    outputs, _ = blocks.max(dim = -1)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "KvgdMAzwU6h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Layer Class"
      ],
      "metadata": {
        "id": "5ivx2yXlVJ4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv_Layer(torch.nn.Module):\n",
        "  def __init__(self, input_channels,kernel_size,n_filters):\n",
        "    super().__init__()\n",
        "    # Defines the size of the square kernels to be used\n",
        "    self.kernel_size = kernel_size\n",
        "    # Defines the number of filters we are using during one convolution\n",
        "    self.n_filters = n_filters\n",
        "    # If using greyscaled images, only one channel will be processed, vs 3 in the case of colored RGB data\n",
        "    self.channels = input_channels\n",
        "    # Defining layer params\n",
        "    self.kernels = torch.nn.Parameter(torch.randn(n_filters,input_channels,kernel_size,kernel_size)*0.1,requires_grad = True)\n",
        "    # In theory you don't need biases, but they can be a useful add on for eking out extra learning\n",
        "    self.biases = torch.nn.Parameter(torch.zeros(n_filters),requires_grad = True)\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    # We save the dims for use\n",
        "    batch_size,input_channels,height,width = inputs.shape\n",
        "    # Pytorch's unfold method is designed for kerneling, which we will be making use of here\n",
        "    # Creates patches that have dims: batch_size, kernel_size * kernel_size * channels, num_patches\n",
        "    blocks = torch.nn.functional.unfold(inputs,kernel_size = self.kernel_size)\n",
        "    # Reshapes the kernels so that they can be dotted with the blocks(batch_size, n_filters, kernel_size * kernel_size *channels)\n",
        "    flattened_kernels = self.kernels.view(self.n_filters,-1)\n",
        "    # Reshapes the blocks to batch_size, num_patches, kernel_size * kernel_size * channels\n",
        "    blocks = blocks.transpose(1,2)\n",
        "    # Multiplies the kernels and the blocks\n",
        "    outputs = torch.matmul(blocks,flattened_kernels.T)\n",
        "    outputs = outputs.transpose(1,2)\n",
        "    # Adds the biases\n",
        "    outputs = outputs + self.biases.view(1, -1, 1)\n",
        "    # Reshapes the output to have the correct dims\n",
        "    out_h = (height - self.kernel_size) + 1\n",
        "    out_w = (width - self.kernel_size) + 1\n",
        "    outputs = outputs.view(batch_size, self.n_filters, out_h, out_w)\n",
        "    # Returns the outputs\n",
        "    return outputs\n",
        "\n",
        "\n",
        "  def backwardsAndOptimize(self,learningRate):\n",
        "    with torch.no_grad():\n",
        "      self.kernels -= learningRate * self.kernels.grad\n",
        "      self.biases -= learningRate * self.biases.grad"
      ],
      "metadata": {
        "id": "aZLHtdJhVRGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Network (Container Class)"
      ],
      "metadata": {
        "id": "YLDp0LuAVXac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This class is mostly just a container for all the other layers. You can instantiate it similarly to how you would a CNN from tensorflow.\n",
        "class Conv_Classifier(torch.nn.Module):\n",
        "  def __init__(self, layers):\n",
        "    super().__init__()\n",
        "    # Saves all of the layers in a ModuleList, allowing easier access to params and grads\n",
        "    self.layers = torch.nn.ModuleList(layers)\n",
        "    # Instantiate the CrossEntropyLoss here\n",
        "    self.loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "  def forward(self,inputs):\n",
        "    outputs = inputs\n",
        "    for i in range(0,len(self.layers)):\n",
        "      # Instantiating with torch.nn.Module makes it so that you don't actually have to call the forward method with .forward(). It is implicitly done.\n",
        "      outputs = self.layers[i](outputs)\n",
        "      # We apply relu everywhere but the final layer\n",
        "      if(isinstance(self.layers[i],(Conv_Layer,Layers_Dense)) and i != len(self.layers)-1):\n",
        "        outputs = torch.nn.functional.relu(outputs)\n",
        "    return outputs\n",
        "  def backwardsAndOptimize(self,learningRate,loss):\n",
        "    # Note: Before this, we could have gotten rid of every single backwardsAndOptimize method and done gradient descent directly by accessing params.\n",
        "    # Since this is an illustrative example, it was not done here.\n",
        "    # Calling loss.backward() computes gradients for every single parameter that contributes to the loss, allowing you to access gradients without passing in loss.\n",
        "    loss.backward()\n",
        "    # Gradients are clipped so we don't get absurdly large gradients which would prevent learning\n",
        "    torch.nn.utils.clip_grad_norm_(self.parameters(), 1.0)\n",
        "    # Does backpropagation for every single model parameter\n",
        "    for i in range(0,len(self.layers)):\n",
        "      if(isinstance(self.layers[i],(Conv_Layer,Layers_Dense))):\n",
        "        self.layers[i].backwardsAndOptimize(learningRate)\n",
        "    for param in self.parameters():\n",
        "      # Because every class that is part of the ModuleList extends torch.nn.Module, we can access their params directly.\n",
        "      if param.grad is not None:\n",
        "        param.grad.zero_()\n",
        "    # Here, we zero the gradients, so that they don't flow between training examples\n",
        "\n",
        "  def predict(self,x,y,batch_size):\n",
        "    # Sets up a predict method that can be used on its own for predictions or for training\n",
        "    # Logits are just the raw output data, while predictions is the predicted class selection(The one that has the highest activation)\n",
        "    logits = self.forward(x)\n",
        "    # If the batch size is 1, we need to handle the prediction differently\n",
        "    if batch_size == 1:\n",
        "      prediction = torch.argmax(logits)\n",
        "    else:\n",
        "      prediction = torch.argmax(logits, dim=1)\n",
        "    # Calculates categorical cross entropy loss, a specialized kind of loss calculation that punishes underconfident probabilities for the correct class\n",
        "    loss = self.loss_fn(logits,y)\n",
        "    return logits, prediction, loss\n",
        "\n",
        "  def train_step(self,x_train,y_train,batch_size,learningRate,num_epochs):\n",
        "    self.train()\n",
        "    # Defining variables that help us change training conditions\n",
        "    self.learningRate = learningRate\n",
        "    for epoch in range(0,num_epochs):\n",
        "      # For this model, we'll be implementing learning rate decay. What this does, is that it decreases the learning rate over time so that the model can\n",
        "      # gradually settle into parameters that minimize the loss while avoiding overfitting(When your model memorizes the training dataset) and having the model keep making large\n",
        "      #  jumps between weights.\n",
        "      if epoch % 10 == 0 and epoch > 0:\n",
        "        self.learningRate = self.learningRate * 0.98\n",
        "      # We permute the dataset so that examples are not learned in the same order each time\n",
        "      permutation = torch.randperm(x_train.shape[0])\n",
        "      epoch_loss = 0.0\n",
        "      for i in range(0,x_train.shape[0],batch_size):\n",
        "        # We select indices for training from the data\n",
        "        indices = permutation[i:i+batch_size]\n",
        "        batch_x, batch_y = x_train[indices], y_train[indices]\n",
        "        # We make predictions on the data\n",
        "        logits, predictions, loss = self.predict(batch_x,batch_y,batch_size)\n",
        "        epoch_loss += loss.item()*batch_x.size(0)\n",
        "        self.backwardsAndOptimize(learningRate,loss)\n",
        "      # We print avg loss per epoch so that a user can monitor the model's loss over time and make decisions based on it\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {epoch_loss / x_train.shape[0]:.4f}\")\n",
        "\n",
        "  def validate(self,x_test,y_test):\n",
        "    self.eval()\n",
        "    # This validation function is meant to be used when assessing the model's ability to generate accurate predictions on testing data(data the model has never seen before)\n",
        "    avg_loss = 0.0\n",
        "    total_correct = 0.0\n",
        "    num_samples = x_test.shape[0]\n",
        "    # We'll store the predictions for later\n",
        "    selections = []\n",
        "    # This implementation is left non vectorized for the sake of readability of output predictions\n",
        "    for i in range(0, x_test.shape[0]):\n",
        "      logits,predictions,loss = self.predict(x_test[i].unsqueeze(0),y_test[i].unsqueeze(0),1) # Added unsqueeze(0) to make it a batch of 1\n",
        "      selections.append(predictions)\n",
        "      avg_loss += loss.item()\n",
        "      if predictions == y_test[i]:\n",
        "        total_correct += 1\n",
        "    # We calculate avg loss and accuracy, useful stats\n",
        "    avg_loss = avg_loss/num_samples\n",
        "    accuracy = total_correct/num_samples\n",
        "    return avg_loss,accuracy"
      ],
      "metadata": {
        "id": "vYqqrlH2VTqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Data"
      ],
      "metadata": {
        "id": "WYQO7cB3VgB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this exercise, we will load the MNIST dataset, a dataset filled with greyscaled images that each have numbers inscribed. The model's job will be to classify the numbers accurately.\n",
        "mnist.load_data(path = \"mnist.csv\")\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = torch.tensor(x_train.astype('float32') / 255.0, dtype=torch.float32).to(\"cuda:0\")\n",
        "x_test = torch.tensor(x_test.astype('float32') / 255.0, dtype=torch.float32).to(\"cuda:0\")\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(\"cuda:0\")\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to(\"cuda:0\")\n",
        "x_train = x_train.unsqueeze(1)\n",
        "x_test = x_test.unsqueeze(1)"
      ],
      "metadata": {
        "id": "MJXtCSbHViUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the Model"
      ],
      "metadata": {
        "id": "RzyhCOruVkPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The model will be defined with it's params here. Feel free to play around with this, but keep in mind that dimension mismatches will cause errors.\n",
        "model = Conv_Classifier([\n",
        "    # Images fed are 28 x 28\n",
        "    Conv_Layer(1,3,32),\n",
        "    Max_Pooling(2,2),\n",
        "    # Images have been scaled down to 14 x 14\n",
        "    Conv_Layer(32,3,64),\n",
        "    Max_Pooling(2,2),\n",
        "    # Flattens the data and feeds it to the MLPs\n",
        "    Conv_Layer(64,3,64),\n",
        "    Flattener(),\n",
        "    # Feeds to MLP layers\n",
        "    Layers_Dense(576,256),\n",
        "    Layers_Dense(256,10)\n",
        "]).to(\"cuda:0\")"
      ],
      "metadata": {
        "id": "KCxJ3S_mVlz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "ASacjOGYVop1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains with a batch_size of 64, learning rate of 0.05, and 5 epochs\n",
        "model.train_step(x_train,y_train,64,0.05,5)"
      ],
      "metadata": {
        "id": "WxNP-8pHVpZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing"
      ],
      "metadata": {
        "id": "EQNYjy61VpiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that the model has been fit on the data, we will test it by having it run on x_test and y_test\n",
        "avg_loss, accuracy = model.validate(x_test,y_test)\n",
        "print(f\"Average Loss: {avg_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "j6Og3479VseP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
